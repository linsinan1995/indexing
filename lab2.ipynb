{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from operator import is_not\n",
    "from functools import partial\n",
    "from keras import backend as k\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Convolution2D, MaxPool2D, Flatten\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.applications import VGG16\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.applications.vgg16 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class imageQuerier:\n",
    "    \"\"\" load the sift_detector as a static member which helps to save time \"\"\"\n",
    "    __sift = cv2.xfeatures2d.SIFT_create()\n",
    "\n",
    "    def __init__(self, images, labels, isTask3 = False):\n",
    "        \"\"\"\n",
    "            @index: images' paths\n",
    "            @labels: images' labels\n",
    "            @isTask3: it is for task3(using CNN instead of VBoW) or not.\n",
    "        \"\"\"\n",
    "        print(\"LOADING IMAGES!\")\n",
    "        # if we just want to build a BoW model, we just need the sift descriptors of them, \n",
    "        # and it helps to improve the speed of querying\n",
    "        if isTask3:\n",
    "            self.images = np.asarray([cv2.resize(cv2.imread(images[i],1), (112, 112), cv2.INTER_LINEAR) for i in tqdm_notebook(range(len(images)))])\n",
    "        else:\n",
    "            self.images = np.asarray([imageQuerier.__sift.detectAndCompute(cv2.imread(images[i],0), None)[1] for i in tqdm_notebook(range(len(images)))])\n",
    "            # image normalization and change the shape from (224, 224, 3, 30000) to (30000, 224, 224, 3)\n",
    "            self.images = preprocess_input(self.image)\n",
    "        self.labels = labels\n",
    "        self.paths = images\n",
    "        self.query_image = None \n",
    "        self.size = len(images)\n",
    "        self.isTask3 = isTask3\n",
    "        self.BOW_init = False\n",
    "\n",
    "    def match(self, index, factor = 0.75):\n",
    "        \"\"\"\n",
    "            match two pictures by their sift descriptors(the query image and image at index i).\n",
    "            \n",
    "            @index : if it is used for task 1, it is a list of two pictures' indeces.\n",
    "                    Otherwise, it is a intergal (the index of the image will be compared)\n",
    "            @factor : the threshold for chosing 'similar' descriptors.\n",
    "        \"\"\"\n",
    "        assert(self.isTask3 == False)\n",
    "        des1 = self.query_descripor\n",
    "        des2 = self.images[index]\n",
    "\n",
    "        if type(des2) == type(None) or len(des2) <= 2:\n",
    "            return 0\n",
    "\n",
    "        # FLANN parameters\n",
    "        FLANN_INDEX_KDTREE = 0\n",
    "        index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 2)\n",
    "        search_params = dict(checks=50) # or pass empty dictionary\n",
    "\n",
    "        flann = cv2.FlannBasedMatcher(index_params,search_params)\n",
    "        matches = flann.knnMatch(des1,des2,k=2)\n",
    "        matchesMask = [[0,0] for i in range(len(matches))]\n",
    "\n",
    "        cnt = 0\n",
    "        # ratio test as per Lowe's paper\n",
    "        for i,(m,n) in enumerate(matches):\n",
    "            if m.distance < factor*n.distance:\n",
    "                matchesMask[i]=[1,0]\n",
    "                cnt += 1\n",
    "        return cnt\n",
    "\n",
    "    def query(self, image, factor = 0.75):\n",
    "        \"\"\"\n",
    "        user interface for image querying(Near Duplicate). \n",
    "        \n",
    "        @image : the image will be queried(in our case, it is the image gets \n",
    "                 geographical transformation)\n",
    "        @factor : a hyperparameter for chosing 'similar' descriptors.\n",
    "      \"\"\"\n",
    "        print(\"Query(SIFT Matcher Retrieval) Start!\")\n",
    "        self.query_image = image\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        _, self.query_descripor = self.__sift.detectAndCompute(gray, None)\n",
    "        \n",
    "        if self.query_descripor is None or len(self.query_descripor) <= 2:\n",
    "            print(\"Entered Picture doesnt have enough key points\")\n",
    "            raise AssertionError\n",
    "\n",
    "        \"\"\" \n",
    "          match every image in the image set, and compute their similarity score\n",
    "          based on the numbers of 'similar' descriptors\n",
    "        \"\"\"\n",
    "        scores = {i:0 for i in range(self.size)}\n",
    "        for i in tqdm_notebook(range(self.size)):\n",
    "            score = self.match(index = i, factor = factor)\n",
    "            scores[i] = score\n",
    "\n",
    "        # This output is just for ploting results.\n",
    "        top_similar_pic_index = sorted(scores,  key=scores.get, reverse = True)\n",
    "        return {idx:scores[idx]/max(scores.values()) for idx in top_similar_pic_index[:12]}\n",
    "    \n",
    "\n",
    "    def BOWquery(self, image, N= 50):\n",
    "        \"\"\"\n",
    "            it is a user interface for using tf-idf weighted BoW querying model. \n",
    "        \n",
    "            @image : the image will be queried\n",
    "            @factor : a theshold for chosing 'similar' descriptors.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.query_image = image\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        _, query_des = self.__sift.detectAndCompute(gray, None)\n",
    "        \n",
    "        if not self.BOW_init:\n",
    "            print(\"Building Vocaburary!\")\n",
    "            self.__build_vocaburary(N)\n",
    "            self.BOW_init = True\n",
    "            \n",
    "        # convert to hist representation\n",
    "        des_to_hist = self.kmeans.predict(query_des)\n",
    "        hist_query_img = np.zeros(N)\n",
    "        for i in des_to_hist:\n",
    "            hist_query_img[i] += 1\n",
    "\n",
    "        # Compute the similarity, and return top 12 similar images\n",
    "\n",
    "        \"\"\" \n",
    "            this lambda function `cos_sim` to compute the consine similarity of two vectors for using \n",
    "            numpy.apply_along_axis(faster than `for` loop). add a small number to make sure denominator \n",
    "            is not equal to 0\n",
    "        \"\"\"\n",
    "        cos_sim = lambda a,b : np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b) + 0.001) \n",
    "\n",
    "        similarity = np.apply_along_axis(partial(cos_sim, hist_query_img), 0, self.weight_voc)\n",
    "        similarity = np.nan_to_num(similarity)\n",
    "        rank_12_index = similarity.argsort()[-12:][::-1]\n",
    "        return {i : similarity[i] for i in rank_12_index}\n",
    "\n",
    "    def __build_vocaburary(self, N):\n",
    "        \"\"\"\n",
    "            Build tf-idf weighted vocaburary/codebook using K-means algorithm\n",
    "\n",
    "            @N : the number of clusters in kmeans\n",
    "        \"\"\"\n",
    "\n",
    "        # drop the images which dont have descriptors(None filter)\n",
    "        list_des = list(filter(None.__ne__, self.images))\n",
    "        descriptors_list = np.vstack(list_des)\n",
    "        self.N = N\n",
    "        self.kmeans = MiniBatchKMeans(n_clusters=N, random_state=0, batch_size=64)\n",
    "        self.kmeans.fit(descriptors_list)\n",
    "        cluster_of_each_descriptor = self.kmeans.predict(descriptors_list)\n",
    "\n",
    "        \"\"\"\n",
    "            images can have different numbers of keypoints, I use a counter(cnt) here \n",
    "            to make sure the descriptors can find its recorresponding image\n",
    "        \"\"\"\n",
    "        cnt = 0 \n",
    "        vocaburary = np.zeros((N, self.size))\n",
    "        for i in range(self.size):\n",
    "            # some images doesnt have descriptors(for task2 we load descriptors instead of image),\n",
    "            # so self.images here is actually images' descriptors\n",
    "            if self.images[i] is None:\n",
    "                continue\n",
    "            n = self.images[i].shape[0]\n",
    "            centered_des = cluster_of_each_descriptor[cnt:(cnt+n)]\n",
    "            for j in centered_des:\n",
    "                vocaburary[j, i] += 1\n",
    "            cnt += n\n",
    "\n",
    "        # TF-IDF weight\n",
    "        tf = vocaburary/(np.sum(vocaburary, 0)+0.1)\n",
    "        idf = np.log(self.size / np.sum(vocaburary, 1))\n",
    "        tf_idf = np.multiply(tf, idf.reshape(-1,1))\n",
    "        self.weight_voc = np.multiply(vocaburary, tf_idf) # tf-idf weighted\n",
    "    \n",
    "    # def __descriptor_to_hist(self, descriptor_vector):\n",
    "    #     return np.argmin(np.linalg.norm(self.center - descriptor_vector, axis=1))\n",
    "\n",
    "    # def __tfidf_weight(self, hist_query_img, n):\n",
    "    #     tf = hist_query_img / (1+hist_query_img)\n",
    "    #     df = (np.sum(self.vocaburary, 1) + 0.5)/ (np.sum(self.vocaburary)+0.5) \n",
    "    #     idf = np.log(1/df)\n",
    "    #     return tf * idf\n",
    "\n",
    "    def build_classifier(self, fit_params):\n",
    "        if self.isTask3:\n",
    "            # Task 3:Using CNN to retrieve images\n",
    "            self.model = VGG16(weights='imagenet', include_top=False, input_shape=(112, 112, 3))\n",
    "\n",
    "            # Freeze the required layers\n",
    "            for layer in self.model.layers[:-4]:\n",
    "                layer.trainable = False\n",
    "\n",
    "            \"\"\"\n",
    "              Add two fully connected layers and a softmax layer \n",
    "              function `.add` doesn't work in my version. solution I use is the link below\n",
    "              https://github.com/keras-team/keras/issues/4040\n",
    "            \"\"\"\n",
    "            last = self.model.output\n",
    "            last = Flatten()(last)\n",
    "            # add two dense layers and softmax layers\n",
    "            last = Dense(2048, activation='relu')(last)\n",
    "            last = Dense(1024, activation='relu')(last)\n",
    "            preds = Dense(43, activation='softmax')(last)\n",
    "            self.model = Model(self.model.input, preds)\n",
    "        else:\n",
    "            # for last step of task2 : Train a neural network to classify several categories\n",
    "            self.model = Sequential([Dense(258, input_dim=self.N, activation=\"relu\"),\n",
    "                                Dense(128, activation=\"relu\"),\n",
    "                                Dense(128, activation=\"relu\"),\n",
    "                                Dense(43, activation=\"softmax\")])\n",
    "        self.__train(fit_params)\n",
    "        \n",
    "    def __train(self, fit_params):\n",
    "        self.model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', \n",
    "                           metrics=['accuracy'])\n",
    "        \n",
    "        \"\"\"\n",
    "            Split data into trainning and validation set. Keras's `shuffle paramater` doen't shuffle \n",
    "            the order of validation set(our images data are in the order of the labels' name).\n",
    "            https://stackoverflow.com/questions/52439468/keras-how-to-take-random-samples-for-validation-set\n",
    "        \"\"\"\n",
    "        if self.isTask3:\n",
    "            XTraining, XValidation, YTraining, YValidation = train_test_split(self.images, self.labels, shuffle = True,\n",
    "                                                             stratify=self.labels,test_size=0.1, random_state=0) \n",
    "        else:\n",
    "            XTraining, XValidation, YTraining, YValidation = train_test_split(self.weight_voc.T, self.labels, shuffle = True,\n",
    "                                                             stratify=self.labels, test_size=0.1, random_state=0) \n",
    "        early_stopping = EarlyStopping(monitor='val_loss',patience=10)\n",
    "        history = self.model.fit(XTraining,YTraining, validation_data=(XValidation,YValidation), **fit_params)\n",
    "        \n",
    "        # Plot training result\n",
    "        return history\n",
    "        \n",
    "\n",
    "    def plot_query_result(self, top_similar_pic, size = (20, 15)):\n",
    "        \"\"\"\n",
    "        It is a function helps to plot the querying result. \n",
    "        \n",
    "        @top_similar_pic : it is a dictionary. Its key is image indexes, and\n",
    "                           its value is the similarity score of that image.\n",
    "        @size : the size of image will be displayed \n",
    "        \"\"\"\n",
    "        plt.figure(figsize=size)\n",
    "        grid = plt.GridSpec(2, 4)\n",
    "        plt.subplot(grid[:2, :2])\n",
    "        plt.imshow(cv2.cvtColor(self.query_image, cv2.COLOR_BGR2RGB))\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(\"Entered Picture\")\n",
    "\n",
    "        keys = list(top_similar_pic.keys())\n",
    "        for i, idx in enumerate(keys[:4]):\n",
    "            plt.subplot(grid[i//2, 2+i%2])\n",
    "            if self.isTask3:\n",
    "                img = cv2.cvtColor(self.images[idx], cv2.COLOR_BGR2RGB)\n",
    "            else:\n",
    "                img = cv2.cvtColor(cv2.imread(self.paths[idx],1 ), cv2.COLOR_BGR2RGB)\n",
    "            plt.imshow(img)\n",
    "            plt.title(\"Top {}, Score {:.2f}\".format(i+5, top_similar_pic[idx]))\n",
    "            plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(figsize=size)\n",
    "        grid = plt.GridSpec(2, 4)\n",
    "        for i, idx in enumerate(keys[4:]):\n",
    "            plt.subplot(grid[i//4, i%4])\n",
    "            if self.isTask3:\n",
    "                img = cv2.cvtColor(self.images[idx], cv2.COLOR_BGR2RGB)\n",
    "            else:\n",
    "                img = cv2.cvtColor(cv2.imread(self.paths[idx],1 ), cv2.COLOR_BGR2RGB)\n",
    "            plt.imshow(img)\n",
    "            plt.title(\"Top {}, Score {:.2f}\".format(i+5, top_similar_pic[idx]))\n",
    "            plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "    @staticmethod\n",
    "    def display(img, size = (20, 15)):\n",
    "        \"\"\"\n",
    "            A function used to plot an image. If `img` is a string, it supposes to be the path.\n",
    "            Otherwise, it's a BGR/gray image\n",
    "\n",
    "            @image : the image will be displayed \n",
    "            @size : the size of image will be displayed \n",
    "        \"\"\"\n",
    "        if isinstance(img, str) :\n",
    "            img = cv2.imread(img, 1)\n",
    "        plt.figure(figsize=size)\n",
    "        plt.axis(\"off\")\n",
    "        if len(img.shape) == 3:\n",
    "            plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB)) \n",
    "        else:\n",
    "            plt.imshow(img,cmap='gray')\n",
    "        plt.show()\n",
    "\n",
    "    @staticmethod\n",
    "    def plot(path, size = (20, 15)):\n",
    "        \"\"\"For debugging\"\"\"\n",
    "        img = cv2.imread(path, 1)\n",
    "        imageQuerier.display(img, size)\n",
    "\n",
    "    @classmethod\n",
    "    def get_sift(cls):\n",
    "        \"\"\"\n",
    "          Get sift detector. It is just for debugging\n",
    "        \"\"\"\n",
    "        return cls.__sift\n",
    "\n",
    "def random_index(_min, _max):\n",
    "    \"\"\"\n",
    "      Get a random number between _min and _max\n",
    "    \"\"\"\n",
    "    np.random.seed(250)\n",
    "    return np.random.randint(_min, _max)\n",
    "\n",
    "def load_data(data_directory):\n",
    "    \"\"\"\n",
    "      Loading the traffic sign data from local path\n",
    "      @data_directory: the path of traffic sign data\n",
    "    \"\"\"\n",
    "    directories = [d for d in os.listdir(data_directory) \n",
    "                   if os.path.isdir(os.path.join(data_directory, d))]\n",
    "    labels = []\n",
    "    images = []\n",
    "    _dict = {int(label):[] for label in directories}\n",
    "    for d in directories:\n",
    "        label_directory = os.path.join(data_directory, d)\n",
    "        file_names = [os.path.join(label_directory, f) \n",
    "                      for f in os.listdir(label_directory) ]\n",
    "        label = int(d)\n",
    "        for filename in file_names:\n",
    "            images.append(filename)\n",
    "            labels.append(label)\n",
    "            _dict[label].append(filename)\n",
    "    return images, labels, _dict\n",
    "\n",
    "def generate_random_image(label_dict):\n",
    "    # get a random picture whitin STOP label\n",
    "    label_idx = 14 # index of STOP\n",
    "    image_idx = random_index(0, len(label_dict[label_idx]))\n",
    "    return cv2.imread(label_dict[label_idx][image_idx], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load images and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ROOT_PATH = \"data\"\n",
    "train_data_directory = os.path.join(ROOT_PATH, \"TrafficSigns/Train\")\n",
    "images_train, labels_train, label_dict = load_data(train_data_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Task1: near duplicate query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image = generate_random_image(label_dict)\n",
    "iq = imageQuerier(images_train, np.array(labels_train), isTask3=False)\n",
    "\n",
    "## get top 12 most similar images in the whole dataset\n",
    "top_similar = iq.query(image)\n",
    "iq.plot_query_result(top_similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Task1: near duplicate query\n",
    "image = generate_random_image(label_dict)\n",
    "iq = imageQuerier(images_train, np.array(labels_train), isTask3=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fit_params = {\n",
    "        \"batch_size\": 128,\n",
    "        \"epochs\": 10,\n",
    "        \"shuffle\":True,\n",
    "        \"verbose\":0\n",
    "        # \"validation_split\": 0.15\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADING IMAGES!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 39209/39209 [01:22<00:00, 477.19it/s]\n"
     ]
    }
   ],
   "source": [
    "iq2 = imageQuerier(images_train, np.array(labels_train), isTask3=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = iq2.build_classifier(fit_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'history'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-4062f26cb3b5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mgrid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGridSpec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Model Accuracy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'history'"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize = (20,16))\n",
    "grid = plt.GridSpec(1, 2)\n",
    "plt.subplot(grid[0,0])\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "\n",
    "plt.subplot(grid[0,1])\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
